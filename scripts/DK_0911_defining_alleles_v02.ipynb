{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Alleles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed at doing allelic comparison between primary contigs and hapltotigs. This was used to generate DataFrames used during the analysis and evolved during the analysis. So it might be a bit more complicated than it needs to be, but it works.\n",
    "\n",
    "Based on original code by Benjamin Schwessinger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook defines alleles based on a Falcon-Unzip assembly where relational information between two contigs are available.\n",
    "\n",
    "#### The input as follows:\n",
    "* Assemblytics alignment of all haplotigs onto their corresponding primary contigs. The *.oriented_coords.csv will be converted to a gff file of haplotig alignments onto their respective primary contigs\n",
    "* gff file of all features. This will be filtered down to genes only and separated out for each contig\n",
    "* protein and gene fa files for reciprocal blast of haplotig sequences onto primary sequences\n",
    "\n",
    "#### What happens:\n",
    "* for each primary gene/protein the the overlapping haplotig is pulled out and added to the blast dataframe\n",
    "* this blast dataframe of primary proteins onto haplotig proteins is than used for filtering to provide the following files\n",
    "* best blast hits are first filtered on e-value minimums followed by BitScore maximum\n",
    "\n",
    "* proteinortho was also used to identify alleles. This was run as follows:\n",
    "\n",
    "/home/gamran/anaconda3/proteinortho_v5.16b/proteinortho5.pl -project=ph_ctg_516 -synteny ph_ctg_516/DK_0911_v04_h_ctg.protein.faa ph_ctg_516/DK_0911_v04_p_ctg.protein.faa\n",
    "\n",
    "* Use the poff-graph and poff-proteinortho-graph as bases for analysis\n",
    "*Â Depending on this run the poff_header needs to be defined. In current case 'Target' followed 'Query'\n",
    "\n",
    "#### The additional input required\n",
    "* is the .poff from proteinortho.\n",
    "\n",
    "\n",
    "#### What comes out in the alleles folder:\n",
    "* DK_0911_v0x_p_ctg.full_df.alleles: contains all proteinortho hits and the best BLAST hits from the primary onto haplotig BLAST\n",
    "* DK_0911_v0x_h_ctg.full_df.alleles: contains all the best BLAST hits from the haplotig onto primary contig BLAST\n",
    "* these files can be filtered based on QCov/%ID in DK_0911_post_allele_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the PATH\n",
    "\n",
    "GENOME_VERSION = 'v032'\n",
    "\n",
    "BASE_OUT_PATH = '/home/gamran/genome_analysis/Warrior/Richard/output/defining_alleles/%s' % GENOME_VERSION\n",
    "GENOME_PATH = '/home/gamran/genome_analysis/Warrior/Richard/output/genome_%s/' % GENOME_VERSION\n",
    "ASSEMBLYTICS_IN_PATH = '/home/gamran/genome_analysis/Warrior/Richard/output/nucmer_assemblytics/%s/Assemblytics/' % GENOME_VERSION\n",
    "\n",
    "BLAST_DB = os.path.join(BASE_OUT_PATH, 'blast_DB')\n",
    "OUT_PATH = os.path.join(BASE_OUT_PATH, 'allele_analysis')\n",
    "OUT_tmp = os.path.join(OUT_PATH, 'tmp')\n",
    "PROTEINORTHO_OUT_PATH = os.path.join(BASE_OUT_PATH, 'proteinortho')\n",
    "    \n",
    "#renamed the allele path to reflect the proteinortho results\n",
    "ALLELE_PATH = os.path.join(OUT_PATH, 'alleles_proteinortho_graph516')\n",
    "if not os.path.exists(BASE_OUT_PATH):\n",
    "    os.mkdir(BASE_OUT_PATH)\n",
    "if not os.path.isdir(OUT_PATH):\n",
    "    os.mkdir(OUT_PATH)\n",
    "if not os.path.isdir(OUT_tmp):\n",
    "    os.mkdir(OUT_tmp)\n",
    "if not os.path.exists(ALLELE_PATH):\n",
    "    os.mkdir(ALLELE_PATH)\n",
    "if not os.path.exists(BLAST_DB):\n",
    "    os.mkdir(BLAST_DB)\n",
    "if not os.path.exists(PROTEINORTHO_OUT_PATH):\n",
    "    os.mkdir(PROTEINORTHO_OUT_PATH)\n",
    "\n",
    "#Define your p and h genome and move it into the allele analysis folder\n",
    "P_GENOME = 'DK_0911_%s_p_ctg' % GENOME_VERSION\n",
    "H_GENOME = 'DK_0911_%s_h_ctg' % GENOME_VERSION\n",
    "for x in (x + '.fa' for x in [P_GENOME, H_GENOME]):\n",
    "    shutil.copy2(GENOME_PATH+'/'+x, OUT_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get proteinortho synteny file which ends with .poff\n",
    "poff_graph_fn = os.path.join(PROTEINORTHO_OUT_PATH, 'ph_ctg_516.poff-graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define ENV parameters for blast hits and threads used in blast analysis\n",
    "n_threads = 16\n",
    "e_value = 1e-3\n",
    "Qcov_cut_off = 0 #this defines the mimimum coverage of the Query to be required for filtering. Will become part of name.\n",
    "PctID_cut_off = 70 #this defines the mimimum PctID accross the alignment to be required for filtering. Will become part of name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_column_p(x, y, z, a, b):\n",
    "    '''Generate attribute column of h on p p_gff dataframe.\n",
    "    '''\n",
    "    string = 'h_contig=%s;query_start=%s;query_stop=%s;query_length=%s;query_aln_ln=%s' % (x, str(y), str(z), str(a), str(b))\n",
    "    return string\n",
    "\n",
    "def att_column_h(x, y, z, a, b):\n",
    "    '''Generate attribute column of h on p h_gff dataframe.\n",
    "    '''\n",
    "    string = 'p_contig=%s;ref_start=%s;ref_stop=%s;ref_length=%s;ref_aln_ln=%s' % (x, str(y), str(z), str(a), str(b))\n",
    "    return string\n",
    "\n",
    "def gene_contig_subset(df, contig, feature):\n",
    "    '''Input the gff and subset the feature columne by gene.\n",
    "        Return data frame subset by contig and gene'''\n",
    "    tmp_df = df[(df[0].str.contains(contig)) & (df[2]==feature)]\n",
    "    tmp_df.sort_values(3,inplace=True)\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    return tmp_df\n",
    "\n",
    "def same_contig_blast(x,y):\n",
    "    '''Function that checks if the blast hit in column y is on the same contig as the the query sequence in\n",
    "    column y.\n",
    "    '''\n",
    "    q_contig = re.search(r'[p|h]([a-z]*_[0-9]*)_?', x).group(1)\n",
    "    if y != 'NaN':\n",
    "        hit_contig = re.search(r'[p|h]([a-z]*_[0-9]*)_?', y).group(1)\n",
    "    else:\n",
    "        hit_contig = 'NaN'\n",
    "    return q_contig == hit_contig\n",
    "\n",
    "def target_on_mapped_haplotig(t_contig, h_contig_overlap):\n",
    "    '''Simple function that checks if the target contig is in the list of h overlapping contigs'''\n",
    "    if t_contig == False or h_contig_overlap == False:\n",
    "        return False\n",
    "    return t_contig in h_contig_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations for blast analysis\n",
    "Do gene blast and protein blast. Initially go off protein blast results. They should be more imformative as it also includes the coding region and the frame.\n",
    "> Pull this in as a dataframe and filter it down as well to each contig. Do blasts both ways as well.\n",
    "> Copy over primary & haplotig .protein.fa and .gene.fa files from the BASE_A_folder, if not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makeblastdb -in DK_0911_v032_h_ctg.gene.fa -dbtype nucl is done!\n",
      "makeblastdb -in DK_0911_v032_p_ctg.protein.fa -dbtype prot is done!\n",
      "makeblastdb -in DK_0911_v032_h_ctg.protein.fa -dbtype prot is done!\n",
      "makeblastdb -in DK_0911_v032_p_ctg.gene.fa -dbtype nucl is done!\n",
      "All databases generated and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#generate the blast databases if not already present\n",
    "os.chdir(BLAST_DB)\n",
    "# copy over .protein.fa and .gene.fa files from the GENOME_PATH folder\n",
    "# if not already present, for both primary and haplotigs\n",
    "shutil.copy2(os.path.join(GENOME_PATH, P_GENOME + '.protein.fa'), BLAST_DB)\n",
    "shutil.copy2(os.path.join(GENOME_PATH, P_GENOME + '.gene.fa'), BLAST_DB)\n",
    "shutil.copy2(os.path.join(GENOME_PATH, H_GENOME + '.protein.fa'), BLAST_DB)\n",
    "shutil.copy2(os.path.join(GENOME_PATH, H_GENOME + '.gene.fa'), BLAST_DB)\n",
    "\n",
    "blast_dir_content = os.listdir(BLAST_DB)\n",
    "for x in blast_dir_content:\n",
    "    if x.endswith('.fa') and ({os.path.isfile(x + e) for e in ['.psq', '.phr', '.pin'] } != {True}\\\n",
    "           and {os.path.isfile(x + e) for e in ['.nin', '.nhr', '.nsq'] } != {True} ):\n",
    "\n",
    "        make_DB_options = ['-in']\n",
    "        make_DB_options.append(x)\n",
    "        make_DB_options.append('-dbtype')\n",
    "        if 'protein' in x:\n",
    "            make_DB_options.append('prot')\n",
    "        else:\n",
    "            make_DB_options.append('nucl')\n",
    "        make_DB_command = 'makeblastdb %s' % ' '.join(make_DB_options)\n",
    "        make_DB_stderr = subprocess.check_output(make_DB_command, shell=True, stderr=subprocess.STDOUT)\n",
    "        print('%s is done!' % make_DB_command)\n",
    "print(\"All databases generated and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blasting DK_0911_v032_p_ctg.gene.fa ...\n",
      "Blast command generated:\n",
      "blastn -query DK_0911_v032_h_ctg.gene.fa -db DK_0911_v032_p_ctg.gene.fa -outfmt 6 -evalue 0.001 -num_threads 16 > /home/gamran/genome_analysis/Warrior/Richard/output/defining_alleles/v032/allele_analysis/DK_0911_v032_h_ctg.DK_0911_v032_p_ctg.0.001.blastn.outfmt6\n",
      "Executing new blast command...\n",
      "New blast executed!\n",
      "\n",
      "Blasting DK_0911_v032_h_ctg.gene.fa ...\n",
      "Blast command generated:\n",
      "blastn -query DK_0911_v032_p_ctg.gene.fa -db DK_0911_v032_h_ctg.gene.fa -outfmt 6 -evalue 0.001 -num_threads 16 > /home/gamran/genome_analysis/Warrior/Richard/output/defining_alleles/v032/allele_analysis/DK_0911_v032_p_ctg.DK_0911_v032_h_ctg.0.001.blastn.outfmt6\n",
      "Executing new blast command...\n",
      "New blast executed!\n",
      "\n",
      "Blasting DK_0911_v032_h_ctg.protein.fa ...\n",
      "Blast command generated:\n",
      "blastp -query DK_0911_v032_p_ctg.protein.fa -db DK_0911_v032_h_ctg.protein.fa -outfmt 6 -evalue 0.001 -num_threads 16 > /home/gamran/genome_analysis/Warrior/Richard/output/defining_alleles/v032/allele_analysis/DK_0911_v032_p_ctg.DK_0911_v032_h_ctg.0.001.blastp.outfmt6\n",
      "Executing new blast command...\n",
      "New blast executed!\n",
      "\n",
      "Blasting DK_0911_v032_p_ctg.protein.fa ...\n",
      "Blast command generated:\n",
      "blastp -query DK_0911_v032_h_ctg.protein.fa -db DK_0911_v032_p_ctg.protein.fa -outfmt 6 -evalue 0.001 -num_threads 16 > /home/gamran/genome_analysis/Warrior/Richard/output/defining_alleles/v032/allele_analysis/DK_0911_v032_h_ctg.DK_0911_v032_p_ctg.0.001.blastp.outfmt6\n",
      "Executing new blast command...\n"
     ]
    }
   ],
   "source": [
    "blast_dict = {}\n",
    "blast_dict['gene_fa'] = [x for x  in os.listdir(BLAST_DB) if x.endswith('gene.fa') and 'ph_ctg' not in x]\n",
    "blast_dict['protein_fa'] = [x for x  in os.listdir(BLAST_DB) if x.endswith('protein.fa') and 'ph_ctg' not in x]\n",
    "blast_stderr_dict = {}\n",
    "\n",
    "for key in blast_dict.keys():\n",
    "    for n, query in enumerate(blast_dict[key]):\n",
    "        tmp_list = blast_dict[key][:]\n",
    "        tmp_stderr_list = []\n",
    "        #this loops through all remaining files and does blast against all those\n",
    "        del tmp_list[n]\n",
    "        for db in tmp_list:\n",
    "            print(\"\\nBlasting %s ...\" %db)\n",
    "            blast_options = ['-query']\n",
    "            blast_options.append(query)\n",
    "            blast_options.append('-db')\n",
    "\n",
    "            blast_options.append(db)\n",
    "            blast_options.append('-outfmt 6')\n",
    "            blast_options.append('-evalue')\n",
    "            blast_options.append(str(e_value))\n",
    "            blast_options.append('-num_threads')\n",
    "            blast_options.append(str(n_threads))\n",
    "            blast_options.append('>')\n",
    "            if 'gene' in query and 'gene' in db:\n",
    "                out_name_list = [ query.split('.')[0], db.split('.')[0], str(e_value), 'blastn.outfmt6']\n",
    "                out_name = os.path.join(OUT_PATH,'.'.join(out_name_list))\n",
    "                blast_options.append(out_name)\n",
    "                blast_command = 'blastn %s' % ' '.join(blast_options)\n",
    "            elif 'protein' in query and 'protein' in db:\n",
    "                out_name_list = [ query.split('.')[0], db.split('.')[0], str(e_value), 'blastp.outfmt6']\n",
    "                out_name = os.path.join(OUT_PATH,'.'.join(out_name_list))\n",
    "                blast_options.append(out_name)\n",
    "                blast_command = 'blastp %s' % ' '.join(blast_options)\n",
    "            print(\"Blast command generated:\\n%s\" % blast_command)\n",
    "            if not os.path.exists(out_name):\n",
    "                print(\"Executing new blast command...\")\n",
    "                blast_stderr_dict[blast_command] = subprocess.check_output(blast_command, shell=True, stderr=subprocess.STDOUT)\n",
    "                print(\"New blast executed!\")\n",
    "            else:\n",
    "                blast_stderr_dict[blast_command] = 'Previously done already!'\n",
    "                print(\"Blast command completed previously!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sequence length dict for all genes and proteins for which blast was performed\n",
    "seq_list = []\n",
    "length_list = []\n",
    "for key in blast_dict.keys():\n",
    "    for file in blast_dict[key]:\n",
    "        for seq in SeqIO.parse(open(file), 'fasta'):\n",
    "            seq_list.append(seq.id)\n",
    "            length_list.append(len(seq.seq))\n",
    "length_dict = dict(zip(seq_list, length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get assemblytics folders\n",
    "ass_folders = [os.path.join(ASSEMBLYTICS_IN_PATH, x) for x in os.listdir(ASSEMBLYTICS_IN_PATH) if x.endswith('_php_8kbp')]\n",
    "ass_folders.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the gff files\n",
    "p_gff = pd.read_csv(os.path.join(GENOME_PATH, P_GENOME+'.anno.gff3'), sep='\\t', header=None)\n",
    "h_gff = pd.read_csv(os.path.join(GENOME_PATH, H_GENOME+'.anno.gff3'), sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_contig_blast(x,y):\n",
    "    '''Function that checks if the blast hit in columne y is on the same contig as the the query sequence in\n",
    "    column y.\n",
    "    '''\n",
    "    try:\n",
    "        q_contig = re.search(r'[p|h]([a-z]*_[0-9]*)_?', x).group(1)\n",
    "        if y != 'NaN':\n",
    "            hit_contig = re.search(r'[p|h]([a-z]*_[0-9]*)_?', y).group(1)\n",
    "        else:\n",
    "            hit_contig = 'NaN'\n",
    "        return q_contig == hit_contig\n",
    "    except AttributeError:\n",
    "        print(\"AttributeError\\nx: %s\\ny: %s\" % (x, y))\n",
    "        sys.exit()\n",
    "    except TypeError:\n",
    "        print(\"x: %s\\ny: %s\" % (x, y))\n",
    "        print(\"TypeError\\ntype(x): %s\\ntype(y): %s\" % (type(x), type(y)))\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the blast df and add QLgth and QCov columns\n",
    "blast_out_dict = {}\n",
    "blast_header = ['Query', 'Target', 'PctID', 'AlnLgth', 'NumMis', 'NumGap', 'StartQuery', 'StopQuery', 'StartTarget',\\\n",
    "              'StopTarget', 'e-value','BitScore']\n",
    "for key in blast_stderr_dict.keys():\n",
    "    file_loc = key.split('>')[-1][1:]\n",
    "    # file_loc = .../output/defining_alleles/allele_analysis/DK_0911_v03_h_ctg.DK_0911_v03_p_ctg.0.001.blastp.outfmt6\n",
    "    \n",
    "    file_name = file_loc.split('/')[-1]\n",
    "    # DK_0911_v03_h_ctg.DK_0911_v03_p_ctg.0.001.blastp.outfmt6\n",
    "    \n",
    "    tmp_df = pd.read_csv(file_loc, sep='\\t', header=None, names=blast_header)\n",
    "\n",
    "    tmp_df['QLgth'] = tmp_df['Query'].apply(lambda x: length_dict[x])\n",
    "    tmp_df['QCov'] = tmp_df['AlnLgth']/tmp_df['QLgth']*100\n",
    "    \n",
    "    tmp_df.sort_values(by=['Query', 'e-value','BitScore', ],ascending=[True, True, False], inplace=True)\n",
    "    \n",
    "    # assert(len(tmp_df.loc[tmp_df['Query'] == 'pcontig_000:10023-10824']) == 0)\n",
    "    \n",
    "    #now make sure to add proteins/genes without blast hit to the DataFrame\n",
    "    #check if gene blast or protein blast and pull out respective query file\n",
    "    if file_loc.split('.')[-2] == 'blastp':\n",
    "        tmp_blast_seq = [x for x in blast_dict['protein_fa'] if x.startswith(file_name.split('.')[0])][0]\n",
    "    elif file_loc.split('.')[-2] == 'blastn':\n",
    "        tmp_blast_seq = [x for x in blast_dict['gene_fa'] if x.startswith(file_name.split('.')[0])][0]\n",
    "    tmp_all_blast_seq = []\n",
    "    #make list of all query sequences\n",
    "    for seq in SeqIO.parse(os.path.join(BLAST_DB, tmp_blast_seq), 'fasta'):\n",
    "        tmp_all_blast_seq.append(str(seq.id))\n",
    "    tmp_all_queries_w_hit = tmp_df[\"Query\"].unique()\n",
    "    tmp_queries_no_hit = set(tmp_all_blast_seq) - set(tmp_all_queries_w_hit)\n",
    "    no_hit_list = []\n",
    "    \n",
    "    #loop over the quieres with no hit and make list of list out of them the first element being the query id\n",
    "    for x in tmp_queries_no_hit:\n",
    "        NA_list = ['NaN'] * len(tmp_df.columns)\n",
    "        NA_list[0] = x\n",
    "        no_hit_list.append(NA_list)\n",
    "    \n",
    "    tmp_no_hit_df = pd.DataFrame(no_hit_list)\n",
    "    tmp_no_hit_df.columns = tmp_df.columns\n",
    "    tmp_no_hit_df['QLgth'] = tmp_no_hit_df.Query.apply(lambda x: length_dict[x])\n",
    "    #map stuff at the tmp level\n",
    "    \n",
    "    tmp_df = tmp_df.append(tmp_no_hit_df)\n",
    "    assert(len(tmp_df.loc[tmp_df['Query'] == 'NaN']) == 0)\n",
    "    \n",
    "    tmp_df['q_contig'] = tmp_df['Query'].str.extract(r'([p|h][a-z]*_[^.]*).?', expand = False)\n",
    "    tmp_df['t_contig'] = tmp_df['Target'].str.extract(r'([p|h][a-z]*_[^.]*).?', expand = False)\n",
    "    # print(tmp_df['q_contig'])\n",
    "    \n",
    "    #fix that if you don't extract anything return False and not 'nan'\n",
    "    tmp_df['t_contig'].fillna(False, inplace=True)\n",
    "    \n",
    "    # print(tmp_df.apply(lambda row: print(row['Query'])), axis = 1)\n",
    "    \n",
    "    tmp_df['q_contig == t_contig'] = tmp_df.apply(lambda row: same_contig_blast(row['Query'], row['Target']), axis = 1)\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "    blast_out_dict[file_name] = tmp_df.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ass_folders:\n",
    "    '''\n",
    "    From oriented_coords.csv files generate gff files with the following set up.\n",
    "    h_contig overlap\n",
    "    'query', 'ref', 'h_feature', 'query_start', 'query_end', 'query_aln_len', 'strand', 'frame', 'attribute_h'\n",
    "    p_contig overlap\n",
    "    'ref', 'query', 'p_feature','ref_start','ref_end', 'alignment_length', 'strand', 'frame', 'attribute_p'.\n",
    "    Save those file in a new folder for further downstream analysis. The same folder should include the contig and gene filtered\n",
    "    gffs. \n",
    "    '''\n",
    "    orient_coords_file = [os.path.join(folder, x) for x in os.listdir(folder) if x.endswith('oriented_coords.csv')][0]\n",
    "    #load in df and generate several additions columns\n",
    "    tmp_df = pd.read_csv(orient_coords_file, sep=',')\n",
    "    #check if there is any resonable alignment if not go to the next one\n",
    "    if len(tmp_df) == 0:\n",
    "        print('Check on %s assemblytics' % (folder))\n",
    "        continue\n",
    "    tmp_df['p_feature'] = \"haplotig\"\n",
    "    tmp_df['h_feature'] ='primary_contig'\n",
    "    tmp_df['strand'] = \"+\"\n",
    "    tmp_df['frame'] = 0\n",
    "    tmp_df['query_aln_len'] = abs(tmp_df['query_end']-tmp_df['query_start'])\n",
    "    tmp_df['alignment_length'] = abs(tmp_df['ref_end'] - tmp_df['ref_start'])\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    tmp_df.sort_values('query', inplace=True)\n",
    "    tmp_df['attribute_p'] = tmp_df.apply(lambda row: att_column_p(row['query'],row['query_start'], row['query_end'],\\\n",
    "                                                         row['query_length'], row['query_aln_len']), axis=1)\n",
    "    \n",
    "    tmp_df['attribute_h'] = tmp_df.apply(lambda row: att_column_h(row['ref'], row['ref_start'], row['ref_end'],\\\n",
    "                                                                  row['ref_length'], row['alignment_length']), axis=1)\n",
    "    \n",
    "    #generate tmp gff dataframe\n",
    "    tmp_p_gff_df = tmp_df.loc[:, ['ref', 'query', 'p_feature','ref_start','ref_end', 'alignment_length', 'strand', 'frame', 'attribute_p']]\n",
    "    #now filter added if start < stop swap round. And if start == 0. This was mostly?! only? the case for haplotigs\n",
    "    tmp_p_gff_df['comp'] = tmp_p_gff_df['ref_start'] - tmp_p_gff_df['ref_end']\n",
    "    tmp_swap_index  = tmp_p_gff_df['comp'] > 0\n",
    "    tmp_p_gff_df.loc[tmp_swap_index, 'ref_start'] , tmp_p_gff_df.loc[tmp_swap_index, 'ref_end'] = tmp_p_gff_df['ref_end'], tmp_p_gff_df['ref_start']\n",
    "    #and if 'ref_start' == g\n",
    "    is_null_index  = tmp_p_gff_df['ref_start'] == 0\n",
    "    tmp_p_gff_df.loc[is_null_index, 'ref_start'] = 1\n",
    "    tmp_p_gff_df = tmp_p_gff_df.iloc[:, 0:9]\n",
    "    tmp_p_gff_df.sort_values('ref_start',inplace=True)\n",
    "    tmp_h_gff_df = tmp_df.loc[:, ['query', 'ref', 'h_feature', 'query_start', 'query_end', 'query_aln_len', 'strand', 'frame', 'attribute_h']]\n",
    "    #same fix for tmp_h_gff\n",
    "    tmp_h_gff_df['comp'] = tmp_h_gff_df['query_start'] - tmp_h_gff_df['query_end']\n",
    "    tmp_swap_index  = tmp_h_gff_df['comp'] > 0\n",
    "    tmp_h_gff_df.loc[tmp_swap_index, 'query_start'] , tmp_h_gff_df.loc[tmp_swap_index, 'query_end'] = tmp_h_gff_df['query_end'], tmp_h_gff_df['query_start']\n",
    "    #and if 'query_start' == g\n",
    "    is_null_index  = tmp_h_gff_df['query_start'] == 0\n",
    "    tmp_h_gff_df.loc[is_null_index, 'query_start'] = 1\n",
    "    tmp_h_gff_df = tmp_h_gff_df.iloc[:, 0:9]\n",
    "    tmp_h_gff_df.sort_values('query_start', inplace=True)\n",
    "    #make the outfolder and save gff files of overlap and filtered gene files\n",
    "    folder_suffix = folder.split('/')[-1]\n",
    "    #get the contig\n",
    "    contig = re.search(r'p([a-z]*_[0-9]*)_', folder_suffix).group(1)\n",
    "    out_folder = os.path.join(OUT_PATH, folder_suffix)\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "    out_name_p = os.path.join(out_folder, folder_suffix )\n",
    "    tmp_p_gff_df.to_csv(out_name_p+'.p_by_h_cov.gff', sep='\\t' ,header=None, index =None)\n",
    "    out_name_h =  os.path.join(out_folder, folder_suffix)\n",
    "    tmp_h_gff_df.to_csv(out_name_h+'.h_by_p_cov.gff' , sep='\\t' ,header=None, index =None)\n",
    "    #write those out to new folder for php together with the p and h gff of genes\n",
    "    p_gene_gff = gene_contig_subset(p_gff, contig, 'gene')\n",
    "    p_gene_gff.to_csv(out_name_p+'.p_gene.gff', sep='\\t' ,header=None, index =None)\n",
    "    h_gene_gff = gene_contig_subset(h_gff, contig, 'gene')\n",
    "    h_gene_gff.to_csv(out_name_h+'.h_gene.gff', sep='\\t' ,header=None, index =None)\n",
    "    \n",
    "    #next would be to do overlap between the gene gff and the corresponding alignment gff. Write this out. Dict for each gene and its corresponding h_contig/p_contig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gene_gff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now loop over the outfolders\n",
    "out_folders = [os.path.join(OUT_PATH, x) for x in os.listdir(OUT_PATH) if x.endswith('_php_8kbp')]\n",
    "out_folders.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gene_h_contig_overlap_dict = {}\n",
    "for out_folder in out_folders:\n",
    "    tmp_folder_content = os.listdir(out_folder)\n",
    "    tmp_p_gene_gff = [os.path.join(out_folder, x) for x in tmp_folder_content if x.endswith('p_gene.gff') ][0]\n",
    "    tmp_p_gene_bed = pybedtools.BedTool(tmp_p_gene_gff).remove_invalid().saveas()\n",
    "    tmp_p_by_h_gff = [os.path.join(out_folder, x) for x in tmp_folder_content if x.endswith('p_by_h_cov.gff') ][0]\n",
    "    tmp_p_by_h_bed = pybedtools.BedTool(tmp_p_by_h_gff).remove_invalid().saveas()\n",
    "    #generate a bed intersect\n",
    "    p_gene_and_p_by_h = tmp_p_gene_bed.intersect(tmp_p_by_h_bed, wb=True)\n",
    "    #check the length of the obtained intersect is \n",
    "    print(\"This is the length of the intersect %i\" %len(p_gene_and_p_by_h ))\n",
    "    if len(p_gene_and_p_by_h) == 0:\n",
    "        continue\n",
    "    #turn this into a dataframe \n",
    "    p_gene_and_p_by_h_df = p_gene_and_p_by_h.to_dataframe()\n",
    "    p_gene_and_p_by_h_df['p_gene'] = p_gene_and_p_by_h_df[8].str.extract(r'ID=([^;]*);', expand=False)\n",
    "    p_gene_and_p_by_h_df['p_protein'] = p_gene_and_p_by_h_df['p_gene'].str.replace('TU', 'model')\n",
    "    #save this in the same folder as dataframe\n",
    "    p_gene_and_p_by_h_df.to_csv(tmp_p_gene_gff.replace('p_gene.gff', 'gene_haplotig_intersect.df'), sep='\\t', index=None)\n",
    "    p_gene_and_p_by_h_df_2 = p_gene_and_p_by_h_df.loc[:, [10, 'p_protein']]\n",
    "    #make a dict for the overlap of each gene with it's correspoding haplotig\n",
    "    tmp_p_gene_h_overlap_df = p_gene_and_p_by_h_df_2.groupby('p_protein')[10].apply(list)\n",
    "    tmp_p_gene_h_overlap_dict = dict(zip(tmp_p_gene_h_overlap_df.index, tmp_p_gene_h_overlap_df.values))\n",
    "    p_gene_h_contig_overlap_dict.update(tmp_p_gene_h_overlap_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h_gene_p_contig_overlap_dict(p_gene_h_contig_overlap_dict):\n",
    "    d = {}\n",
    "    for key in p_gene_h_contig_overlap_dict.keys():\n",
    "        for val in p_gene_h_contig_overlap_dict[key]:\n",
    "            if val in d:\n",
    "                d[val].append(key)\n",
    "            else:\n",
    "                d[val] = [key]\n",
    "    return d\n",
    "h_gene_p_contig_overlap_dict = get_h_gene_p_contig_overlap_dict(p_gene_h_contig_overlap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get out the p_protein on h_protein blast out dataframe and appand the h_contig overlap for each protein\n",
    "_key = [x for x in blast_out_dict.keys() if x.startswith(P_GENOME) and x.split('.')[-2] == 'blastp' and x.endswith('outfmt6')][0]\n",
    "p_gene_h_contig_overlap_df = pd.DataFrame([p_gene_h_contig_overlap_dict.keys(), p_gene_h_contig_overlap_dict.values()],  index = ['p_protein', 'h_contig_overlap']).T\n",
    "_tmp_df = blast_out_dict[_key]\n",
    "_tmp_df = _tmp_df.merge(p_gene_h_contig_overlap_df, how='outer' ,left_on='Query', right_on='p_protein')\n",
    "_tmp_df['h_contig_overlap'].fillna(False, inplace=True)\n",
    "#check if the protein hit resites on the overlapping haplotig\n",
    "_tmp_df['t_contig == h_contig_overlap'] = _tmp_df.apply(lambda row: target_on_mapped_haplotig(row['t_contig'], row['h_contig_overlap']), axis =1 )\n",
    "#blast_out_dict[_key] = _tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_gene_h_contig_overlap_df['p_protein'] = p_gene_h_contig_overlap_df['p_protein'].apply(lambda x: x.replace('model', 'TU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the dataframe again\n",
    "_tmp_df.to_csv(os.path.join(OUT_PATH, _key+'.allele_analysis'), index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qcov_cut_off = ''\n",
    "#PctID_cut_off = ''\n",
    "if Qcov_cut_off and PctID_cut_off:\n",
    "    pass\n",
    "else:\n",
    "    print('No cut off defined! Do you want to define it. Please go ahead above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/gamran/genome_analysis/Warrior/Richard/scripts')\n",
    "%run 'DK_0911_proteinortho.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poff_graph_header = ['Target', 'Query', 'evalue_ab', 'bitscore_ab', 'evalue_ba', 'bitscore_ba', 'same_strand' , 'simscore']\n",
    "po_df = pd.read_csv(poff_graph_fn, sep='\\t', header=None, names=poff_graph_header, comment='#' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduceGroups(g, labels, maxBest):\n",
    "    '''returns the best hit based on e-value and BitScore per group'''\n",
    "    for label, maxBest in zip(labels, maxBest):\n",
    "        if len(g) == 1:\n",
    "            return g\n",
    "        if maxBest:\n",
    "            g = g[g[label] == g[label].max()]\n",
    "        else:\n",
    "            g = g[g[label] == g[label].min()]\n",
    "#     if len(g) > 1:\n",
    "#         print('Could not reduce group to 1 element. %s ' % g['Query'])\n",
    "    return g\n",
    "\n",
    "# _tmp_df = _tmp_df.groupby('Query').apply(lambda g: reduceGroups(g, ['e-value', 'BitScore'], [False, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no add a comparision column to po_df and _tmp_df\n",
    "po_df['comp'] = po_df['Query'] + po_df['Target']\n",
    "_tmp_df['comp'] = _tmp_df['Query'] + _tmp_df['Target']\n",
    "#generate a new allele_source column\n",
    "_tmp_df['allele_source'] = 'BLAST'\n",
    "_tmp_df.loc[_tmp_df[_tmp_df.comp.isin(po_df.comp.unique())].index, 'allele_source' ] = 'PO'\n",
    "#now drop the comp column again\n",
    "_tmp_df = _tmp_df.drop('comp', 1)\n",
    "\n",
    "print(_tmp_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(_tmp_df))\n",
    "no_PO_df = _tmp_df[_tmp_df['allele_source'] == 'BLAST']\n",
    "PO_df = _tmp_df[_tmp_df['allele_source'] == 'PO']\n",
    "\n",
    "no_PO_df = no_PO_df.groupby('Query').apply(lambda g: reduceGroups(g, ['e-value', 'BitScore'], [False, True]))\n",
    "_tmp_df = no_PO_df.append(PO_df, ignore_index=True)\n",
    "print(len(_tmp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_df.to_csv(os.path.join(ALLELE_PATH, P_GENOME + '.full_df.alleles'), sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htgOnPctgBlast = pd.read_csv(os.path.join(OUT_PATH, '%s.%s.0.001.blastp.outfmt6' % (H_GENOME, P_GENOME)), sep='\\t', index_col=None, header=None, names=blast_header)\n",
    "htgOnPctgBlast['QLgth'] = htgOnPctgBlast['Query'].apply(lambda x: length_dict[x])\n",
    "htgOnPctgBlast['QCov'] = htgOnPctgBlast['AlnLgth']/htgOnPctgBlast['QLgth']*100\n",
    "\n",
    "htgOnPctgBlast['q_contig'] = htgOnPctgBlast['Query'].str.extract(r'([p|h][a-z]*_[^.]*).?', expand = False)\n",
    "htgOnPctgBlast['t_contig'] = htgOnPctgBlast['Target'].str.extract(r'([p|h][a-z]*_[^.]*).?', expand = False)\n",
    "\n",
    "htgOnPctgBlast['q_contig == t_contig'] = htgOnPctgBlast.apply(lambda row: same_contig_blast(row['Query'], row['Target']), axis = 1)\n",
    "htgOnPctgBlast.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#get out the p_protein on h_protein blast out dataframe and appand the h_contig overlap for each protein\n",
    "p_gene_h_contig_overlap_df = pd.DataFrame([h_gene_p_contig_overlap_dict.keys(), h_gene_p_contig_overlap_dict.values()],  index = ['p_protein', 'h_contig_overlap']).T\n",
    "\n",
    "htgOnPctgBlast = htgOnPctgBlast.merge(p_gene_h_contig_overlap_df, how='outer' ,left_on='q_contig', right_on='p_protein')\n",
    "htgOnPctgBlast['h_contig_overlap'].fillna(False, inplace=True)\n",
    "#check if the protein hit resides on the overlapping haplotig\n",
    "\n",
    "htgOnPctgBlast['t_contig == h_contig_overlap'] = htgOnPctgBlast.apply(lambda row: target_on_mapped_haplotig(row['Target'], row['h_contig_overlap']), axis = 1)\n",
    "htgOnPctgBlast['allele_source'] = 'h_rBLAST'\n",
    "htgOnPctgBlast = htgOnPctgBlast.groupby('Query').apply(lambda g: reduceGroups(g, ['e-value', 'BitScore'], [False, True]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htgOnPctgBlast.to_csv(os.path.join(ALLELE_PATH, H_GENOME + '.full_df.alleles'), sep='\\t', index=None, header=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
