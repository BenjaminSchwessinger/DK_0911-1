{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DK_0911_post_allele_analysis_v02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on original code by Benjamin Schwessinger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: output from `DK_0911_defining_alleles_v02` & primary+haplotig (ph) protein/gene/cds .*fasta* files from `DK_0911_generate_fasta_files_from_gff3`.\n",
    "- Programs: **MUSCLE**, **PAML**\n",
    "- Purpose: generate and save a DataFrame containing dN/dS information (number of nonsynonymous substitutions per non-synonymous site to the number of synonymous substitutions per synonymous site), as well as Hamming & Levenshtein distances (measures of % identity). Also provides visualisations of some of this data.\n",
    "\n",
    "#### Overview\n",
    "1. Reads in the large allele DataFrames generated in `DK_0911_defining_alleles_v02` (i.e. proteinortho hits OR best blast hit) - see description header cell in that notebook for more information on which alleles are included in that DataFrame.\n",
    "2. Filters the allele DataFrames based on %ID and %QCov (this can be set to filter only BLAST-identified alleles or both BLAST- and proteinortho-identified alleles) so that distance information is not calculated on an unnecessarily large number of alleles.\n",
    "3. Calculates distance & dN/dS information, and saves this to an output file so that it does not have to be re-calculated (if for whatever reason, the inputs change so that dN/dS or distance information should change, this output file (`DK_0911_v0x_analysed_alleles.df`) should be deleted so that it can be re-generated.\n",
    "4. Plots graphs of allele-type distribution (pie chart) and allele-type Levenshtein distances (measures of similarity) for different levels of allele-filtering (QCov/TCov/%ID/Levenshtein similarity).\n",
    "\n",
    "NB:\n",
    "- dN/dS information is currently not utilised in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:13.301271Z",
     "start_time": "2018-02-21T08:36:12.743801Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.100608Z",
     "start_time": "2018-02-21T08:36:13.304541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import distance\n",
    "import editdistance\n",
    "import math\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import collections\n",
    "import pybedtools\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.158691Z",
     "start_time": "2018-02-21T08:36:15.103684Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables here\n",
    "GENOME_VERSION = 'v04'\n",
    "\n",
    "BASE_PATH = '/home/benjamin/genome_assembly/Warrior/allele_analysis/%s/' % GENOME_VERSION\n",
    "\n",
    "YN00_PATH = '/home/gamran/genome_analysis/Warrior/Richard/output/post_analysis/yn00.ctl'\n",
    "BASE_OUT_PATH = os.path.join(BASE_PATH, 'post_analysis/')\n",
    "ALLELE_PATH = os.path.join(BASE_PATH, 'allele_analysis/alleles_proteinortho_graph516/')\n",
    "UNFILTERED_DF_PATH = os.path.join(BASE_PATH, \\\n",
    "    'allele_analysis/DK_0911_%s_p_ctg.DK_0911_%s_h_ctg.0.001.blastp.outfmt6.allele_analysis' \\\n",
    "                                  % (GENOME_VERSION, GENOME_VERSION))\n",
    "GENOME_PATH = '/home/benjamin/genome_assembly/Warrior/genome_%s/' % GENOME_VERSION\n",
    "FIGURE_PATH = os.path.join(BASE_OUT_PATH, 'figures')\n",
    "\n",
    "GENOME = 'DK_0911_%s' % GENOME_VERSION\n",
    "P_GENOME = GENOME + '_p_ctg'\n",
    "H_GENOME = GENOME + '_h_ctg'\n",
    "\n",
    "threads = 8\n",
    "\n",
    "# Base filtering so that distance calculations are not performed on all allele pairs.\n",
    "# Distance calculations will only be performed on allele pairs above the defined cutoffs.\n",
    "# Note that proteinortho alleles will not be affected (this can be changed in the filterAlleleDf function).\n",
    "BASE_QCOV_CUTOFF = 0\n",
    "BASE_TCOV_CUTOFF = 0\n",
    "BASE_PCTID_CUTOFF = 0\n",
    "\n",
    "P_PROTEINS_FASTA = os.path.join(GENOME_PATH, P_GENOME + '.protein.fa')\n",
    "\n",
    "PAML_PATH = os.path.join(BASE_OUT_PATH, 'paml')\n",
    "if not os.path.exists(BASE_OUT_PATH):\n",
    "    os.mkdir(BASE_OUT_PATH)\n",
    "if not os.path.exists(FIGURE_PATH):\n",
    "    os.mkdir(FIGURE_PATH)\n",
    "if not os.path.exists(PAML_PATH):\n",
    "    os.mkdir(PAML_PATH)\n",
    "    shutil.copy2(YN00_PATH, PAML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.170805Z",
     "start_time": "2018-02-21T08:36:15.162908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COV_PATH = '/home/benjamin/genome_assembly/Warrior/COV'\n",
    "homo_bed_fh = os.path.join(COV_PATH, 'DK0911_v04_ph_ctg.ph_p_homo_cov.bed')\n",
    "anno_gff_p_fh = os.path.join(GENOME_PATH, 'DK_0911_v04_p_ctg.anno.gff3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.185324Z",
     "start_time": "2018-02-21T08:36:15.175796Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PH_PROTEIN_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.protein.fa')\n",
    "PH_GENE_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.gene.fa')\n",
    "PH_CDS_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.cds.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.210063Z",
     "start_time": "2018-02-21T08:36:15.190688Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allgffTogenegff(gff_fh, write_out=True):\n",
    "    '''Converts at complete gff to a gene gff only and writtes it out.'''\n",
    "    gene_gff = pd.read_csv(gff_fh, sep='\\t', header=None)\n",
    "    gene_gff = gene_gff[gene_gff[2] == 'gene']\n",
    "    gene_gff.reset_index(drop=True, inplace=True)\n",
    "    gene_gff.to_csv(gff_fh.replace('anno', 'gene'), sep='\\t', header=False, index=False)\n",
    "    return gene_gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.242440Z",
     "start_time": "2018-02-21T08:36:15.215258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_8_id(x):\n",
    "    '''Function that pulls out the ID from the 9th column of a df.'''\n",
    "    pattern = r'ID=([a-zA-Z0-9_.]*);'\n",
    "    regex = re.compile(pattern)  \n",
    "    m = regex.search(x)\n",
    "    match = m.groups()[0].replace('TU', 'model')\n",
    "    if match.startswith('cds.'):\n",
    "        match = match[4:]\n",
    "    if 'exon' in match:\n",
    "        _list = match.split('.')\n",
    "        match = '.'.join(_list[:-1])\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.352592Z",
     "start_time": "2018-02-21T08:36:15.247098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignMatchType(allele_source, overlap, no_overlap):\n",
    "    if not allele_source == np.nan:\n",
    "        return allele_source\n",
    "    \n",
    "    s = allele_source + '_'\n",
    "    \n",
    "    if overlap:\n",
    "        s += 'overlap'\n",
    "    elif no_overlap:\n",
    "        s += 'no_overlap'\n",
    "    else: # different_pcontig\n",
    "        s += 'unlinked'\n",
    "    return s\n",
    "\n",
    "def reduceGroups(g):\n",
    "    '''returns the best hit based on e-value and BitScore per group'''\n",
    "    if len(g) == 1:\n",
    "        return g\n",
    "    tmp_g = g[g['e-value'] == g['e-value'].min()]\n",
    "    if len(tmp_g) == 1:\n",
    "        return tmp_g\n",
    "    return tmp_g[tmp_g['BitScore'] == tmp_g['BitScore'].max()]\n",
    "\n",
    "def filterAlleleDf(alleleDf, qCov, tCov, pctId, levSim, leavePO=False):\n",
    "    if leavePO:\n",
    "        no_PO_df = alleleDf[(alleleDf['allele_source'] == 'h_rBLAST') | (alleleDf['allele_source'] == 'BLAST')]\n",
    "        PO_df = alleleDf[alleleDf['allele_source'] == 'PO']\n",
    "\n",
    "        filtered_no_PO_df = filterAlleleDf(no_PO_df, qCov, tCov, pctId, levSim)\n",
    "        return filtered_no_PO_df.append(PO_df, ignore_index=True)\n",
    "    \n",
    "    if qCov:\n",
    "        alleleDf = alleleDf[alleleDf['QCov'] > qCov]\n",
    "    if tCov:\n",
    "        alleleDf = alleleDf[alleleDf['TCov'] > tCov]\n",
    "    if pctId:\n",
    "        alleleDf = alleleDf[alleleDf['PctID'] > pctId]\n",
    "    if levSim:\n",
    "        levDist = (100-levSim)/100.0\n",
    "        alleleDf = alleleDf[alleleDf['protein_levenshtein'] < levDist]\n",
    "\n",
    "    return alleleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.382032Z",
     "start_time": "2018-02-21T08:36:15.357181Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geneUnphased(Gene_gff_fh, Homo_cov_bed_fh ):\n",
    "    \"\"\"\n",
    "    Returns a list of all genes that are unphased.\n",
    "    \n",
    "    Input: * Fh for annotation gff file\n",
    "           * Fh for Homo_cov_bed_fh\n",
    "    Output: A set of gene IDs that are unphases\n",
    "    \"\"\"\n",
    "    geneGff_bed = pybedtools.BedTool(Gene_gff_fh)\n",
    "    homo_p_bed = pybedtools.BedTool(Homo_cov_bed_fh)\n",
    "    gene_ids_ph_p_homo = []\n",
    "    for x in geneGff_bed.intersect(homo_p_bed, f=0.4):\n",
    "        y = col_8_id(x[8])\n",
    "        gene_ids_ph_p_homo.append(y)\n",
    "    gene_ids_ph_p_homo = set(gene_ids_ph_p_homo)\n",
    "    return gene_ids_ph_p_homo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.406941Z",
     "start_time": "2018-02-21T08:36:15.386074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_unphased(alleleDf, gff_fh=anno_gff_p_fh, homo_bed_fh=homo_bed_fh):\n",
    "    alleleDf = alleleDf.copy()\n",
    "    _ = allgffTogenegff(gff_fh)\n",
    "    Unphasedgenes = geneUnphased(gff_fh.replace('anno', 'gene'), homo_bed_fh)\n",
    "    alleleDf['unphased'] = False\n",
    "    alleleDf.loc[alleleDf.Query.isin(Unphasedgenes), 'unphased'] = True\n",
    "    #print(len(Unphasedgenes))\n",
    "    return alleleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:15.686729Z",
     "start_time": "2018-02-21T08:36:15.678115Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFastaDict(fastaFile):\n",
    "    d = {}\n",
    "    for gene in SeqIO.parse(fastaFile, 'fasta'):\n",
    "        d[gene.id] = gene\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:16.428394Z",
     "start_time": "2018-02-21T08:36:16.350536Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeAllelicFasta(alleleOne, alleleTwo, alleleType, outPath):\n",
    "    '''writes fasta file containing fasta information for two alleles\n",
    "    in the outPath'''\n",
    "    assert(alleleType.upper() in ['CDS', 'GENE', 'PROTEIN'])\n",
    "    \n",
    "    seqRecordDict = globals()['SEQRECORD_' + alleleType.upper() + '_DICT']\n",
    "    try:\n",
    "        alleleSeqRecords = [seqRecordDict[alleleOne], seqRecordDict[alleleTwo]]\n",
    "    except KeyError:\n",
    "        print(alleleOne)\n",
    "        print(alleleTwo)\n",
    "        print(alleleType)\n",
    "        sys.exit()\n",
    "    with open(os.path.join(outPath, alleleType.lower() + '.fa'), 'w') as outFile:\n",
    "        SeqIO.write(alleleSeqRecords, outFile, 'fasta')\n",
    "    return True\n",
    "\n",
    "def writeAlignmentScript(alleleOutPath, scriptLoc = os.path.join(PAML_PATH, 'paml_script.sh')):\n",
    "    with open(scriptLoc, 'a') as outFile:\n",
    "        print('cd %s' % alleleOutPath, file=outFile)\n",
    "        print('/home/gamran/anaconda3/muscle3.8.31_i86linux64 -clwstrict -in protein.fa -out protein.aln', file=outFile)\n",
    "        print('perl /home/gamran/anaconda3/pal2nal.v14/pal2nal.pl -output paml protein.aln cds.fa > cds_codon.aln', file=outFile)\n",
    "        print('perl /home/gamran/anaconda3/pal2nal.v14/pal2nal.pl protein.aln cds.fa > cds_codon.clustal', file=outFile)\n",
    "        print('cp %s/yn00.ctl ./' % PAML_PATH, file=outFile)\n",
    "        print('/home/gamran/anaconda3/paml4.9g/bin/yn00', file=outFile)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:16.992441Z",
     "start_time": "2018-02-21T08:36:16.942926Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareAlignmentBashScript(scriptLoc = os.path.join(PAML_PATH, 'paml_script.sh')):\n",
    "    with open(scriptLoc, 'w') as pamlScript:\n",
    "        print('#!/bin/bash', file=pamlScript)\n",
    "\n",
    "    for index, [Query, Target] in alleleDf.iloc[:, :2].iterrows():\n",
    "        #if we don't have a blast hit skip.\n",
    "        if pd.isnull(Target):\n",
    "            continue\n",
    "        else:\n",
    "            alleleOutPath = os.path.join(PAML_PATH, '%s_%s' % (Query, Target))\n",
    "            if not os.path.exists(alleleOutPath):\n",
    "                os.mkdir(os.path.join(PAML_PATH, '%s_%s' % (Query, Target)))\n",
    "\n",
    "            writeAllelicFasta(Query, Target, 'CDS', alleleOutPath)\n",
    "            writeAllelicFasta(Query, Target, 'PROTEIN', alleleOutPath)\n",
    "\n",
    "            writeAlignmentScript(alleleOutPath, os.path.join(PAML_PATH, 'paml_script.sh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:38:54.239672Z",
     "start_time": "2018-02-21T08:38:54.033946Z"
    }
   },
   "outputs": [],
   "source": [
    "def assignDistancesToAlleles(folder, alignmentFile, alleleType):\n",
    "    '''Adds Hamming and Levenshtein distance columns to an allele pair\n",
    "    (indexed by 'folder' name) in df'''\n",
    "    #print(folder)\n",
    "    if pd.isnull(folder):\n",
    "        return np.nan, np.nan\n",
    "    assert(alleleType.upper() in ['PROTEIN', 'CDS', 'GENE'])\n",
    "    seq1, seq2 = AlignIO.read(open(alignmentFile, 'r'), format='clustal', seq_count=2)\n",
    "    seq1 = str(seq1.seq).upper()\n",
    "    seq2 = str(seq2.seq).upper()\n",
    "    assert(len(seq1) == len(seq2))\n",
    "    return editdistance.eval(seq1, seq2)/len(seq1), distance.hamming(seq1, seq2, normalized=True)\n",
    "\n",
    "def assignDistancesToAllAlleles(df_folder_index, df, tmp_path):\n",
    "    \"\"\"\n",
    "    Reads in the index that contains the folder pairings for the alignements.\n",
    "    Returns a protein_df and CDS_df that contain the hamming and levenshtein distance each.\n",
    "    \"\"\"\n",
    "    cleaned_index = [x for x in df_folder_index if str(x) != 'nan']\n",
    "    df = df.loc[cleaned_index, :]\n",
    "    df = df.loc[df.index.dropna(), :]\n",
    "    #print(df.index)\n",
    "    count = 0\n",
    "    total = len(df_folder_index)\n",
    "    percentDone = 0\n",
    "    protein_lev_dict = {}\n",
    "    protein_ham_dict = {}\n",
    "    CDS_lev_dict = {}\n",
    "    CDS_ham_dict = {}\n",
    "    \n",
    "    print(\"Calculating distances and adding them to the allele DataFrame...\")\n",
    "    \n",
    "    for folder in df.index:\n",
    "        if pd.isnull(folder):\n",
    "            proteinAlignmentFile = ''\n",
    "            cdsAlignmentFile = ''\n",
    "        else:\n",
    "            proteinAlignmentFile = os.path.join(PAML_PATH, folder, 'protein.aln')\n",
    "            cdsAlignmentFile = os.path.join(PAML_PATH, folder, 'cds_codon.clustal')\n",
    "        #here the nan get overwritten. This doesn't matter though as they are all\n",
    "        #nan anyway.\n",
    "        protein_lev_dict[folder], protein_ham_dict[folder]  = \\\n",
    "        assignDistancesToAlleles(folder, proteinAlignmentFile, 'PROTEIN')\n",
    "        CDS_lev_dict[folder], CDS_ham_dict[folder]  = \\\n",
    "        assignDistancesToAlleles(folder, cdsAlignmentFile, 'CDS')\n",
    "\n",
    "        count += 1\n",
    "        #if round(count/total * 100) > percentDone:\n",
    "            #percentDone = round(count/total * 100)\n",
    "            #print(\"%s%% complete\" % percentDone)\n",
    "            \n",
    "    newdf_columns=['protein_hamming', 'protein_levenshtein', 'cds_hamming',\n",
    "       'cds_levenshtein']\n",
    "    if len(protein_ham_dict) > 0:\n",
    "        df = pd.DataFrame([protein_ham_dict,protein_lev_dict,CDS_ham_dict,CDS_lev_dict]).T\n",
    "        df.rename(columns=dict(zip(df.columns,newdf_columns)),inplace=True)\n",
    "        out_name = os.path.join(tmp_path, '%s_%s.dftmp' % (df.index[0],df.index[-1]))\n",
    "        df.to_csv(out_name, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:40:56.511550Z",
     "start_time": "2018-02-21T08:40:56.380654Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dNdS_to_df(line, alleleDf, folder, dNdS_label):\n",
    "    dN = re.findall(r'dN = [-| ]?(.*) w', line)[0]\n",
    "    dS = re.findall(r'dS = [-| ]?(.*) dN', line)[0]\n",
    "    return assign_dNdS(dN, dS, alleleDf, folder, dNdS_label)\n",
    "\n",
    "def assign_dNdS(dN, dS, alleleDf, folder, dNdS_label):\n",
    "    if float(dS) > 0:\n",
    "        alleleDf.loc[folder, dNdS_label] = float(dN)/float(dS)\n",
    "    else:\n",
    "        alleleDf.loc[folder, dNdS_label] = np.nan\n",
    "    return alleleDf\n",
    "\n",
    "def assign_dNdS_to_all_alleles(alleleDf):\n",
    "    for folder in alleleDf.index:\n",
    "        if pd.isnull(folder):\n",
    "            continue\n",
    "        alleleYn = os.path.join(PAML_PATH, folder,'yn.out')\n",
    "        with open(alleleYn, 'r') as ynOut:\n",
    "            #now loop over the lines and parse out stuff\n",
    "            for i, line in enumerate(ynOut):\n",
    "                if line.startswith('seq. seq. ') and i > 0:\n",
    "                    next(ynOut) # we want the line that is two after the line starting with 'seq. seq '\n",
    "                    dataLine = next(ynOut)\n",
    "                    dN = dataLine.split('+-')[0].rstrip().split(' ')[-1]\n",
    "                    dS = dataLine.split('+-')[1].rstrip().split(' ')[-1]\n",
    "                    alleleDf = assign_dNdS(dN, dS, alleleDf, folder, 'yn00_dN/dS')\n",
    "                elif line.startswith('LWL85:') and 'nan' not in line:\n",
    "                    alleleDf = parse_dNdS_to_df(line, alleleDf, folder, 'LWL85_dN/dS')\n",
    "                elif line.startswith('LWL85m:') and 'nan' not in line:\n",
    "                    alleleDf = parse_dNdS_to_df(line, alleleDf, folder, 'LWL85m_dN/dS')\n",
    "                elif line.startswith('LPB93:') and 'nan' not in line:\n",
    "                    alleleDf = parse_dNdS_to_df(line, alleleDf, folder, 'LPB93_dN/dS')\n",
    "                else:\n",
    "                    continue\n",
    "    return alleleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:19.271770Z",
     "start_time": "2018-02-21T08:36:19.235533Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkPamlFilesExist(alleleDf):\n",
    "    '''loops through all folder names in alleleDf.index to check if their PAML files have\n",
    "    all been generated in those folders. refDict is based on the contents of a folder\n",
    "    that was known to be run successfully.'''\n",
    "    refDict = {'aln': 2,\n",
    "     'clustal': 1,\n",
    "     'ctl': 1,\n",
    "     'dN': 1,\n",
    "     'dS': 1,\n",
    "     'fa': 2,\n",
    "     'out': 1,\n",
    "     'rst': 1,\n",
    "     'rst1': 1,\n",
    "     'rub': 1,\n",
    "     't': 1}\n",
    "    for file in (x for x in alleleDf.index if not pd.isnull(x)):\n",
    "        if not os.path.exists(os.path.join(PAML_PATH, file)):\n",
    "            return False\n",
    "        discrepancies = getDiscrepancies(os.path.join(PAML_PATH, file), refDict)\n",
    "        if discrepancies != '':\n",
    "            print(discrepancies)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:19.699976Z",
     "start_time": "2018-02-21T08:36:19.691458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return it.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:22.313931Z",
     "start_time": "2018-02-21T08:36:20.443700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hFullAlleleDf = pd.read_csv(os.path.join(ALLELE_PATH, '%s.full_df.alleles' % H_GENOME), header=0, sep='\\t')\n",
    "hFullAlleleDf['matchType'] = pd.Series(index=hFullAlleleDf.index)\n",
    "hFullAlleleDf['matchType'] = hFullAlleleDf.apply(lambda row: assignMatchType(row['allele_source'], row['t_contig == h_contig_overlap'], row['q_contig == t_contig']), axis=1)\n",
    "\n",
    "pFullAlleleDf = pd.read_csv(os.path.join(ALLELE_PATH, '%s.full_df.alleles' % P_GENOME), header=0, sep='\\t')\n",
    "pFullAlleleDf['matchType'] = pd.Series(index=pFullAlleleDf.index)\n",
    "pFullAlleleDf['matchType'] = pFullAlleleDf.apply(lambda row: assignMatchType(row['allele_source'], row['t_contig == h_contig_overlap'], row['q_contig == t_contig']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:22.478208Z",
     "start_time": "2018-02-21T08:36:22.317052Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out haplotig proteins that already have alleles identified by BLAST or proteinortho.\n",
    "hFullAlleleDf = hFullAlleleDf[(~hFullAlleleDf['Query'].isin(pFullAlleleDf['Target']))]\n",
    "pFullAlleleDf['aQuery'] = pFullAlleleDf['Query']\n",
    "pFullAlleleDf['aTarget'] = pFullAlleleDf['Target']\n",
    "hFullAlleleDf['aQuery'] = hFullAlleleDf['Target']\n",
    "hFullAlleleDf['aTarget'] = hFullAlleleDf['Query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:22.506018Z",
     "start_time": "2018-02-21T08:36:22.480899Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phFullAlleleDf = pFullAlleleDf.append(hFullAlleleDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:25.765746Z",
     "start_time": "2018-02-21T08:36:22.508274Z"
    }
   },
   "outputs": [],
   "source": [
    "SEQRECORD_PROTEIN_DICT = getFastaDict(PH_PROTEIN_FASTA)\n",
    "SEQRECORD_GENE_DICT = getFastaDict(PH_GENE_FASTA)\n",
    "SEQRECORD_CDS_DICT = getFastaDict(PH_CDS_FASTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:55:37.320369Z",
     "start_time": "2018-02-21T08:55:37.274502Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alleleDf = phFullAlleleDf\n",
    "alleleDf['folder'] = alleleDf.Query + '_' + alleleDf.Target\n",
    "alleleDf.set_index('folder', inplace=True)\n",
    "# assert(len(alleleDf) == len(overlapDf) + len(noOverlapDf) + len(diffContigDf) + len(manualAssignDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T08:36:26.123973Z",
     "start_time": "2018-02-21T08:36:25.812163Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/gamran/genome_analysis/Warrior/Richard/scripts')\n",
    "%run file_counting.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:16:51.587661Z",
     "start_time": "2018-02-21T09:16:51.363894Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(alleleDf = alleleDf):\n",
    "    prepareAlignmentBashScript(os.path.join(PAML_PATH, 'paml_script.sh'))\n",
    "    \n",
    "    # if already run before, comment out this line\n",
    "    print(\"Checking whether all PAML files already exist in %s...\" % PAML_PATH)\n",
    "    if checkPamlFilesExist(alleleDf):\n",
    "        print('PAML appears to have been run to completion previously. Therefore, it will not be run this time.')\n",
    "    else:\n",
    "        'Not all files generated by PAML appear to exist. Running PAML now (this may take some time)...'\n",
    "        !bash {os.path.join(PAML_PATH, 'paml_script.sh')}\n",
    "        print('PAML finished running.')\n",
    "\n",
    "    analysedAllelesPath = os.path.join(BASE_OUT_PATH, GENOME+'_analysed_alleles.df')\n",
    "    #if os.path.exists(analysedAllelesPath) and os.path.getsize(analysedAllelesPath) > 0:\n",
    "    #    print(\"DataFrame with distance calculations at %s appears to have already been generated. Reading in this dataframe instead of re-generating it.\" % analysedAllelesPath)\n",
    "     #   alleleDf = pd.read_csv(analysedAllelesPath, sep='\\t', index_col=0)\n",
    "    \n",
    "    alleleDf.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    \n",
    "    #generate a tmp folder for the parallized analysis\n",
    "    tmp_path = os.path.join(BASE_OUT_PATH, 'tmp')\n",
    "    if not os.path.exists(tmp_path):\n",
    "        os.mkdir(tmp_path)\n",
    "    \n",
    "    #do parallized analysis\n",
    "    Parallel(n_jobs=threads)(delayed(assignDistancesToAllAlleles)(list(folder_index_list),alleleDf, tmp_path)\\\n",
    "                       for folder_index_list in grouper(alleleDf.index, 100, np.nan))\n",
    "    #pull results in again form saved out files\n",
    "    tmp_assigneddfs_fh = [os.path.join(tmp_path, file) for file in os.listdir(tmp_path)\\\n",
    "                         if file.endswith('dftmp') ]\n",
    "    \n",
    "    #include an assertion as well to check if all worked allright\n",
    "    #assert(len(tmp_assigneddfs_fh)==\\\n",
    "         #  (int(alleleDf.loc[alleleDf.index.dropna(),:].shape[0]/100)+1))\n",
    "    \n",
    "    distdf_header = ['protein_hamming', 'protein_levenshtein', 'cds_hamming',\n",
    "       'cds_levenshtein']\n",
    "    distdf = pd.DataFrame(columns=distdf_header)\n",
    "    for df_fh in tmp_assigneddfs_fh:\n",
    "        distdf = pd.concat([distdf, pd.read_csv(df_fh, header=None, names=distdf_header , sep='\\t')])\n",
    "    distdf['Index'] = distdf.index\n",
    "    alleleDf['Index'] = alleleDf.index\n",
    "    tmp_df = pd.merge(alleleDf.loc[alleleDf.index.dropna(),:], distdf.loc[distdf.index.dropna(),:],how='inner')\n",
    "    alleleDf = pd.concat([tmp_df, alleleDf.loc[alleleDf.index.isnull(),:]])\n",
    "\n",
    "    \n",
    "    #now clean up again\n",
    "    for file in tmp_assigneddfs_fh:\n",
    "        os.remove(file)\n",
    "\n",
    "    alleleDf.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    pd.util.testing.assert_frame_equal(alleleDf, pd.read_csv(analysedAllelesPath, sep='\\t', index_col=0))\n",
    "    \n",
    "    alleleDf = assign_dNdS_to_all_alleles(alleleDf)\n",
    "    \n",
    "    aleleDf = assign_unphased(alleleDf)\n",
    "    \n",
    "    alleleDf.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    \n",
    "    return alleleDf, distdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-21T09:16:51.854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether all PAML files already exist in /home/benjamin/genome_assembly/Warrior/allele_analysis/v04/post_analysis/paml...\n",
      "PAML appears to have been run to completion previously. Therefore, it will not be run this time.\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n",
      "Calculating distances and adding them to the allele DataFrame...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    alleleDf, distdf = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-21T09:17:36.725Z"
    }
   },
   "outputs": [],
   "source": [
    "len(phFullAlleleDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-21T09:17:37.393Z"
    }
   },
   "outputs": [],
   "source": [
    "len(alleleDf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:49:47.072255Z",
     "start_time": "2018-02-20T02:49:46.220172Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## FIGURE PLOTTING ##########\n",
    "\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}%\\n({v:d})'.format(p=pct,v=val)\n",
    "    return my_autopct\n",
    "\n",
    "def autolabel(rects, labels, ax, fontsize):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for i, rect in enumerate(rects):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., height, str(labels[i]), ha='center', va='bottom', fontsize=fontsize)\n",
    "\n",
    "def getNumNoAlleles(pProteinFastaFile, alleleDf):\n",
    "    with open(pProteinFastaFile) as pProteinFasta:\n",
    "        pProteinList = []\n",
    "        for line in pProteinFasta:\n",
    "            if line.startswith('>'):\n",
    "                pProteinList.append(line[1:].strip())\n",
    "\n",
    "    assert(len(pProteinList) == len(set(pProteinList)))\n",
    "\n",
    "\n",
    "    pairedPProteinList = list(alleleDf['Query'])\n",
    "    pairedPProteinList += list(alleleDf['Target'])\n",
    "    pairedPProteinList = set(pairedPProteinList)\n",
    "    \n",
    "    for pairedPProtein in pairedPProteinList:\n",
    "        if pairedPProtein in pProteinList:\n",
    "            pProteinList.remove(pairedPProtein)\n",
    "    \n",
    "    return len(pProteinList)\n",
    "\n",
    "def plotAlleleTypesPie(ax, alleleDf, colors, includeNoAlleles=True):\n",
    "    '''Plots a pie chart of allele types, with the option of also including \n",
    "    primary proteins with no alleles. Strictly, this is not an accurate representation\n",
    "    of the distribution of primary proteins as the reciprocal BLAST-identified (h on p) alleles\n",
    "    may result in double-counting of primary proteins.\n",
    "    '''\n",
    "    # OrderedDict to preserve order, so that plots are coloured with same key as the distance \n",
    "    # bar graphs. This is a bit of a hack-fix; must enter these by hand again in the same order \n",
    "    # as 'matchType' occurs in the alleleAveragesByMatchType DataFrame.\n",
    "    alleleTypeCountDict = collections.OrderedDict()\n",
    "    \n",
    "    for matchType in alleleDf['matchType'].unique():\n",
    "        alleleTypeCountDict[matchType] = len(alleleDf[alleleDf['matchType'] == matchType])\n",
    "    \n",
    "    if includeNoAlleles==True:\n",
    "        numNoAlleles = getNumNoAlleles(P_PROTEINS_FASTA, alleleDf)\n",
    "        alleleTypeCountDict['no_allele'] = numNoAlleles\n",
    "\n",
    "    patches, texts, autotexts = ax.pie(list(alleleTypeCountDict.values()), labels=alleleTypeCountDict.keys(), autopct=make_autopct(list(alleleTypeCountDict.values())), colors=colors)\n",
    "    ax.axis('equal')\n",
    "    ax.set_title('Allele Types', loc='center', fontsize=TITLE_SIZE, position=(0.5, 1.1))\n",
    "\n",
    "def plotLevenshteinBar(alleleAverages, ax, colors):\n",
    "    '''Plots a bar graph of normalised Levenshtein distances on ax from DataFrame alleleAverages.'''\n",
    "    \n",
    "    ind = np.arange(len(alleleAverages.protein_levenshtein))\n",
    "    rects = ax.bar(ind, alleleAverages.protein_levenshtein, 0.35, color=colors, align='center') \n",
    "    \n",
    "    sns.despine(top=True, right=True)\n",
    "    \n",
    "    barLabels = []\n",
    "    for levDist in alleleAverages.protein_levenshtein:\n",
    "        barLabels.append(str(int((1-levDist)*100)) + '%')\n",
    "    autolabel(rects, barLabels, ax, INLINE_LABEL_SIZE)\n",
    "\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(alleleAverages.index, rotation=45)\n",
    "\n",
    "    # ax.set_xlabel('Allele Types', fontsize=AXIS_LABEL_SIZE)\n",
    "    ax.set_ylabel('Normalised Levenshtein Distance', fontsize=AXIS_LABEL_SIZE)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=AXIS_TICK_SIZE, pad=3)\n",
    "\n",
    "    for tick in ax.get_xaxis().get_major_ticks():\n",
    "        tick.set_pad(2*tick.get_pad())\n",
    "        tick.label1 = tick._get_text1()\n",
    "        \n",
    "def plotAlleles(alleleDf, qCovFilters, tCovFilters, pctIdFilters, levSimFilters, leavePO):\n",
    "    '''Makes a 3x2 plot with normalised Levenshtein distance plots in column 1 and\n",
    "    a pie chart representing the distribution of allele types in column 2.\n",
    "    Each row shows different levels of filtering.\n",
    "    \n",
    "    leavePO is a boolean that determines whether only BLAST hits will be filtered (leavePO=True)\n",
    "    or both BLAST and PO alleles should be filtered (leavePO=False)'''\n",
    "    cmap = plt.cm.Greens\n",
    "    colors = cmap(np.linspace(0.0, 0.6, len(alleleDf['matchType'].unique())))\n",
    "    \n",
    "    assert(len(qCovFilters) == len(pctIdFilters) == len(levSimFilters))\n",
    "    \n",
    "    fig, ax = plt.subplots(len(qCovFilters), 2, figsize=(30, 12*len(qCovFilters)))\n",
    "    \n",
    "    for i in range(len(qCovFilters)):\n",
    "        \n",
    "        filteredAlleleDf = filterAlleleDf(alleleDf, qCovFilters[i], tCovFilters[i], pctIdFilters[i], levSimFilters[i], True)\n",
    "        print(filteredAlleleDf[filteredAlleleDf.allele_source == 'PO'].shape[0])\n",
    "        # levenshtein distance plot\n",
    "        alleleAveragesByMatchType = filteredAlleleDf.groupby(['matchType']).mean()\n",
    "        plotLevenshteinBar(alleleAveragesByMatchType, ax[i, 0], colors)\n",
    "        ax[i, 0].set_xticklabels(alleleAveragesByMatchType.index, rotation=45, ha='right')\n",
    "        \n",
    "        # pie plot\n",
    "        plotAlleleTypesPie(ax[i, 1], filteredAlleleDf, colors)\n",
    "        \n",
    "        # include filtering criteria in title\n",
    "        qCovFilter = qCovFilters[i]\n",
    "        tCovFilter = tCovFilters[i]\n",
    "        pctIdFilter = pctIdFilters[i]\n",
    "        levSimFilter = levSimFilters[i]\n",
    "        if qCovFilter < BASE_QCOV_CUTOFF:\n",
    "            qCovFilter = BASE_QCOV_CUTOFF\n",
    "            print('Base QCov cut-off is 70%; if you desire to filter below this value, decrease BASE_QCOV_CUTOFF.')\n",
    "        if tCovFilter < BASE_TCOV_CUTOFF:\n",
    "            tCovFilter = BASE_TCOV_CUTOFF\n",
    "            print('Base TCov cut-off is 70%; if you desire to filter below this value, decrease BASE_TCOV_CUTOFF.')\n",
    "        if pctIdFilter < BASE_PCTID_CUTOFF:\n",
    "            pctIdFilter = BASE_PCTID_CUTOFF\n",
    "            print('Base %ID cut-off is 70%; if you desire to filter below this value, decrease BASE_PCTID_CUTOFF.')\n",
    "        if not levSimFilter:\n",
    "            levSimFilter = 0\n",
    "    \n",
    "        ax[i, 0].set_title('QCov > %s%%, TCov > %s%%, ID > %s%%, L. sim. > %s%%, PO Filtered: %s' % (qCovFilter, tCovFilter, pctIdFilter, levSimFilter, not leavePO), position=(0.5, 0.85))\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(FIGURE_PATH, 'fig'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:50:17.346988Z",
     "start_time": "2018-02-20T02:49:49.617825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Used in the pie chart for all text except title\n",
    "# the ax.pie plotting interface is weird - cannot set other font sizes properly?\n",
    "mpl.rcParams['font.size'] = 24\n",
    "\n",
    "TITLE_SIZE = 32\n",
    "AXIS_LABEL_SIZE = 28\n",
    "AXIS_TICK_SIZE = 24\n",
    "INLINE_LABEL_SIZE = 24\n",
    "\n",
    "# These lists must all be the same length\n",
    "qCovFilters = [False, 80, 90, 95]\n",
    "pctIdFilters = [False, 80, 90, 95]\n",
    "tCovFilters = [False, 80, 90, 95]\n",
    "levSimFilters = [False, False, False, False]\n",
    "\n",
    "plotAlleles(alleleDf, qCovFilters, tCovFilters, pctIdFilters, levSimFilters, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
